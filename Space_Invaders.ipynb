{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Space Invaders.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "SLnm3MMrIMQQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U0neBfX5JR5Z",
        "colab_type": "code",
        "outputId": "9c9707d8-75a1-4984-f4c3-ab3637cb0a24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install gym-retro"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym-retro in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-retro) (0.10.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.11.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.3.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (2018.11.29)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (1.22)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym->gym-retro) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3K6bQPw0MZRZ",
        "colab_type": "code",
        "outputId": "b258bc47-1ae7-4b13-eba4-057acd514bfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1802
        }
      },
      "cell_type": "code",
      "source": [
        "!wget http://www.atarimania.com/roms/Roms.rar\n",
        "!unrar x Roms.rar\n",
        "!unzip ./Roms/ROMS.zip\n",
        "%cd ROMS\n",
        "!python -m retro.import ."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-03 11:53:14--  http://www.atarimania.com/roms/Roms.rar\n",
            "Resolving www.atarimania.com (www.atarimania.com)... 195.154.81.199\n",
            "Connecting to www.atarimania.com (www.atarimania.com)|195.154.81.199|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10823448 (10M) [application/x-rar-compressed]\n",
            "Saving to: ‘Roms.rar.2’\n",
            "\n",
            "Roms.rar.2          100%[===================>]  10.32M   621KB/s    in 18s     \n",
            "\n",
            "2019-01-03 11:53:31 (599 KB/s) - ‘Roms.rar.2’ saved [10823448/10823448]\n",
            "\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from Roms.rar\n",
            "\n",
            "\n",
            "Would you like to replace the existing file Roms/HC ROMS.zip\n",
            "11729845 bytes, modified on 2018-12-24 16:58\n",
            "with a new one\n",
            "11729845 bytes, modified on 2018-12-24 16:58\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit N\n",
            "\n",
            "\n",
            "Would you like to replace the existing file Roms/ROMS.zip\n",
            "7826740 bytes, modified on 2018-12-24 16:58\n",
            "with a new one\n",
            "7826740 bytes, modified on 2018-12-24 16:58\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit N\n",
            "\n",
            "All OK\n",
            "Archive:  ./Roms/ROMS.zip\n",
            "replace ROMS/128 in 1 Game Select ROM (128 in 1) (Unknown) ~.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "/content/ROMS\n",
            "Importing 1905 potential games...\n",
            "Importing UpNDown-Atari2600\n",
            "Importing YarsRevenge-Atari2600\n",
            "Importing Qbert-Atari2600\n",
            "Importing Gopher-Atari2600\n",
            "Importing Assault-Atari2600\n",
            "Importing Krull-Atari2600\n",
            "Importing BattleZone-Atari2600\n",
            "Importing Centipede-Atari2600\n",
            "Importing CrazyClimber-Atari2600\n",
            "Importing BeamRider-Atari2600\n",
            "Importing Kangaroo-Atari2600\n",
            "Importing Gravitar-Atari2600\n",
            "Importing Seaquest-Atari2600\n",
            "Importing Qbert-Atari2600\n",
            "Importing Assault-Atari2600\n",
            "Importing Berzerk-Atari2600\n",
            "Importing BankHeist-Atari2600\n",
            "Importing Alien-Atari2600\n",
            "Importing DoubleDunk-Atari2600\n",
            "Importing JourneyEscape-Atari2600\n",
            "Importing Frostbite-Atari2600\n",
            "Importing Riverraid-Atari2600\n",
            "Importing VideoPinball-Atari2600\n",
            "Importing Phoenix-Atari2600\n",
            "Importing MontezumaRevenge-Atari2600\n",
            "Importing Pong-Atari2600\n",
            "Importing MsPacman-Atari2600\n",
            "Importing Tutankham-Atari2600\n",
            "Importing ChopperCommand-Atari2600\n",
            "Importing NameThisGame-Atari2600\n",
            "Importing Pitfall-Atari2600\n",
            "Importing WizardOfWor-Atari2600\n",
            "Importing KungFuMaster-Atari2600\n",
            "Importing Amidar-Atari2600\n",
            "Importing Asterix-Atari2600\n",
            "Importing PrivateEye-Atari2600\n",
            "Importing Hero-Atari2600\n",
            "Importing Skiing-Atari2600\n",
            "Importing Kaboom-Atari2600\n",
            "Importing Carnival-Atari2600\n",
            "Importing Enduro-Atari2600\n",
            "Importing FishingDerby-Atari2600\n",
            "Importing Jamesbond-Atari2600\n",
            "Importing Boxing-Atari2600\n",
            "Importing Zaxxon-Atari2600\n",
            "Importing Defender-Atari2600\n",
            "Importing Atlantis-Atari2600\n",
            "Importing StarGunner-Atari2600\n",
            "Importing Solaris-Atari2600\n",
            "Importing Assault-Atari2600\n",
            "Importing DemonAttack-Atari2600\n",
            "Importing Bowling-Atari2600\n",
            "Importing IceHockey-Atari2600\n",
            "Importing Freeway-Atari2600\n",
            "Importing SpaceInvaders-Atari2600\n",
            "Importing Pooyan-Atari2600\n",
            "Importing Adventure-Atari2600\n",
            "Importing Breakout-Atari2600\n",
            "Importing Venture-Atari2600\n",
            "Importing Qbert-Atari2600\n",
            "Importing RoadRunner-Atari2600\n",
            "Importing AirRaid-Atari2600\n",
            "Importing Tennis-Atari2600\n",
            "Importing TimePilot-Atari2600\n",
            "Importing Asteroids-Atari2600\n",
            "Importing Robotank-Atari2600\n",
            "Imported 66 games\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z3yuL-eBIuBB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import retro\n",
        "\n",
        "from skimage import transform\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import random\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sIh1jw1eJPLw",
        "colab_type": "code",
        "outputId": "903d07b4-e65d-43c6-c249-dd8a3fe1d591",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "env = retro.make(game='SpaceInvaders-Atari2600')\n",
        "\n",
        "print('The size of our frame is: ', env.observation_space)\n",
        "print('The action size is: ', env.action_space.n)\n",
        "\n",
        "possible_actions = np.array(np.identity(env.action_space.n, dtype=int).tolist())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of our frame is:  Box(210, 160, 3)\n",
            "The action size is:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_DF5XRSfKlIw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_frame(frame):\n",
        "  gray = rgb2gray(frame)\n",
        "  cropped_frame = gray[8:-12, 4:-12]\n",
        "  normalized_frame = cropped_frame/255.0\n",
        "  preprocessed_frame = transform.resize(normalized_frame, [110,84])\n",
        "  return preprocessed_frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0sz34aj1zsFW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stack_size = 4\n",
        "\n",
        "stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "  frame = preprocess_frame(state)\n",
        "  \n",
        "  if is_new_episode:\n",
        "    stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "    \n",
        "    stacked_frames.append(frame)\n",
        "    stacked_frames.append(frame)\n",
        "    stacked_frames.append(frame)\n",
        "    stacked_frames.append(frame)\n",
        "    \n",
        "    stacked_state = np.stack(stacked_frames, axis=2)\n",
        "    \n",
        "  else:\n",
        "    stacked_frames.append(frame)\n",
        "    \n",
        "    stacked_state = np.stack(stacked_frames, axis=2)\n",
        "    \n",
        "  return stacked_state, stacked_frames\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QePrXYm109v2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "state_size = [110, 84, 4]\n",
        "\n",
        "action_size = env.action_space.n\n",
        "learning_rate = 0.00025           # alpha\n",
        "\n",
        "total_episodes = 50\n",
        "max_steps = 50000\n",
        "batch_size = 64\n",
        "\n",
        "explore_start = 1.0\n",
        "explore_stop = 0.01\n",
        "decay_rate = 0.00001\n",
        "\n",
        "gamma = 0.9\n",
        "\n",
        "pretrain_length = batch_size # no. of experiences stored in the memory when initialized for the first time\n",
        "memory_size = 1000000 # no. of experiences the memory can keep\n",
        "\n",
        "stack_size = 4\n",
        "\n",
        "training = True # false to just see the trained agent\n",
        "episode_render = False # true if you want to render the env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_vO6xLqJ2Zdf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQNetwork:\n",
        "  def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.learning_rate = learning_rate\n",
        "    \n",
        "    with tf.variable_scope(name):\n",
        "      \n",
        "      self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
        "      self.actions_ = tf.placeholder(tf.float32, [None, self.action_size], name=\"actions_\")\n",
        "      \n",
        "      # target_Q = R(s,a) + ymax Qhat(s', a')\n",
        "      self.target_Q = tf.placeholder(tf.float32, [None], name=\"targets\")\n",
        "      \n",
        "      self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
        "                                 filters = 32,\n",
        "                                 kernel_size = [8,8],\n",
        "                                 strides = [4,4],\n",
        "                                 padding = \"VALID\",\n",
        "                                 kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "                                 name = \"conv1\")\n",
        "      \n",
        "      self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\")\n",
        "      \n",
        "      self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
        "                                   filters = 64,\n",
        "                                   kernel_size=[4,4],\n",
        "                                   strides = [2,2],\n",
        "                                   padding = \"VALID\",\n",
        "                                   kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "                                   name = \"conv2\")\n",
        "      \n",
        "      self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\")\n",
        "      \n",
        "      self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
        "                                   filters = 64,\n",
        "                                   kernel_size= [3,3],\n",
        "                                   strides = [2,2],\n",
        "                                   padding = \"VALID\",\n",
        "                                   kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "                                   name = \"conv3\")\n",
        "      self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\")\n",
        "      \n",
        "      self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
        "      \n",
        "      self.fc = tf.layers.dense(inputs = self.flatten,\n",
        "                               units = 512,\n",
        "                               activation = tf.nn.elu,\n",
        "                               kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
        "                               name = \"fc1\")\n",
        "      \n",
        "      self.output = tf.layers.dense(inputs = self.fc,\n",
        "                                   kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
        "                                   units = self.action_size,\n",
        "                                   activation = None)\n",
        "      self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_))\n",
        "      \n",
        "      self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
        "      \n",
        "      self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2EyWuQs6eEm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MI0_6oKT6tve",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Memory():\n",
        "  def __init__(self, max_size):\n",
        "    self.buffer = deque(maxlen = max_size)\n",
        "    \n",
        "  def add(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "    \n",
        "  def sample(self, batch_size):\n",
        "    buffer_size = len(self.buffer)\n",
        "    index = np.random.choice(np.arange(buffer_size),\n",
        "                            size = batch_size,\n",
        "                            replace = False)\n",
        "    return [self.buffer[i] for i in index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jibM9nRi8GAn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "memory = Memory(max_size = memory_size)\n",
        "for i in range(pretrain_length):\n",
        "  \n",
        "  if i ==0:\n",
        "    state = env.reset()\n",
        "    \n",
        "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "    \n",
        "  choice = random.randint(1, len(possible_actions))-1\n",
        "  action = possible_actions[choice]\n",
        "  next_state, reward, done, _ = env.step(action)\n",
        "  \n",
        "  next_state, stacked_frames = stack_frames(stacked_frames, next_state, True)\n",
        "  \n",
        "  if done:\n",
        "    next_state = np.zeros(state.shape)\n",
        "    \n",
        "    memory.add((state, action, reward, next_state, done))\n",
        "    \n",
        "    stae = env.reset()\n",
        "    \n",
        "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "    \n",
        "  else:\n",
        "    memory.add((state, action, reward, next_state, done))\n",
        "    \n",
        "    state = next_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hrLdFSSE9y8m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
        "\n",
        "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
        "\n",
        "write_op = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mnebu9eO-h_O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
        "  \n",
        "  exp_exp_tradeoff = np.random.rand()\n",
        "  \n",
        "  explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
        "  \n",
        "  if(explore_probability > exp_exp_tradeoff):\n",
        "    choice = random.randint(1, len(possible_actions)) -1 \n",
        "    action = possible_actions[choice]\n",
        "    \n",
        "  else:\n",
        "    \n",
        "    Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
        "    \n",
        "    choice = np.argmax(Qs)\n",
        "    action = possible_actions[choice]\n",
        "    \n",
        "  return action, explore_probability"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3C4KcZnD_l_v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1033
        },
        "outputId": "ee0a41d9-83dd-4c71-adf5-41967d99df5b"
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver()\n",
        "rewards_list = []\n",
        "\n",
        "if training == True:\n",
        "  with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    decay_step = 0\n",
        "    \n",
        "    for episode in range(total_episodes):\n",
        "      step = 0\n",
        "      episode_rewards = []\n",
        "      state = env.reset()\n",
        "      \n",
        "      state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "      \n",
        "      while step < max_steps:\n",
        "        step += 1\n",
        "        \n",
        "        decay_step += 1\n",
        "        \n",
        "        action, explore_probability = predict_action(explore_start, explore_stop, decay_rate,\n",
        "                                                     decay_step, state, possible_actions)\n",
        "        \n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        if episode_render:\n",
        "          env.render()\n",
        "          \n",
        "        episode_rewards.append(reward)\n",
        "          \n",
        "        if done:\n",
        "          next_state = np.zeros((110,84), dtype=np.int)\n",
        "          \n",
        "          next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "          \n",
        "          step = max_steps\n",
        "          \n",
        "          total_reward = np.sum(episode_rewards)\n",
        "          \n",
        "          print('Episode: {}'.format(episode),\n",
        "               'Total Reward: {}'.format(total_reward),\n",
        "               'Explore P: {:.4f}'.format(explore_probability),\n",
        "               'Training Loss {:.4f}'.format(loss))\n",
        "          \n",
        "          rewards_list.append((episode, total_reward))\n",
        "          \n",
        "          memory.add((state, action, reward, next_state, done))\n",
        "        else:\n",
        "          next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "          \n",
        "          memory.add((state, action, reward, next_state, done))\n",
        "          \n",
        "          state = next_state\n",
        "          \n",
        "        batch = memory.sample(batch_size)\n",
        "        states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
        "        actions_mb = np.array([each[1] for each in batch])\n",
        "        rewards_mb = np.array([each[2] for each in batch])\n",
        "        next_states_mb = np.array([each[3] for each in batch])\n",
        "        dones_mb = np.array([each[4] for each in batch])\n",
        "        \n",
        "        target_Qs_batch = []\n",
        "        \n",
        "        Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
        "        \n",
        "        for i in range (0, len(batch)):\n",
        "          terminal = dones_mb[i]\n",
        "          if terminal:\n",
        "            target_Qs_batch.append(rewards_mb[i])\n",
        "            \n",
        "          else:\n",
        "            target = rewards_mb[i] + gamma*np.max(Qs_next_state[i])\n",
        "            target_Qs_batch.append(target)\n",
        "            \n",
        "        targets_mb = np.array([each for each in target_Qs_batch])\n",
        "        \n",
        "        loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
        "                          feed_dict={DQNetwork.inputs_: states_mb,\n",
        "                                    DQNetwork.target_Q: targets_mb,\n",
        "                                    DQNetwork.actions_: actions_mb})\n",
        "        \n",
        "        summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
        "                                               DQNetwork.target_Q: targets_mb,\n",
        "                                               DQNetwork.actions_: actions_mb})\n",
        "        \n",
        "        writer.add_summary(summary, episode)\n",
        "        writer.flush()\n",
        "        \n",
        "      if episode % 5 == 0:\n",
        "        save_path = saver.save(sess, \"./models/model.ckpt\")\n",
        "        print(\"Model Saved\")\n",
        "          "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 0 Total Reward: 120.0 Explore P: 0.9774 Training Loss 3.4796\n",
            "Model Saved\n",
            "Episode: 1 Total Reward: 35.0 Explore P: 0.9631 Training Loss 0.0034\n",
            "Episode: 2 Total Reward: 255.0 Explore P: 0.9363 Training Loss 0.0045\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-894b478f1961>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m                           feed_dict={DQNetwork.inputs_: states_mb,\n\u001b[1;32m     80\u001b[0m                                     \u001b[0mDQNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_Q\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargets_mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                                     DQNetwork.actions_: actions_mb})\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "2M7-KfJ1gV16",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b5497296-7852-40e9-b16f-4129faded878"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  total_test_rewards = []\n",
        "  \n",
        "  saver.restore(sess, \"./models/model.ckpt\")\n",
        "  \n",
        "  for episode in range(1):\n",
        "    total_rewards = 0\n",
        "    \n",
        "    state = env.reset()\n",
        "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "    \n",
        "    print(\"******************************************************\")\n",
        "    print(\"EPISODE \", episode)\n",
        "    while True:\n",
        "      \n",
        "      state = state.reshape((1, *state_size))\n",
        "      \n",
        "      Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state})\n",
        "      \n",
        "      choice = np.argmax(Qs)\n",
        "      action = possible_actions[choice]\n",
        "      \n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      env.render()\n",
        "      \n",
        "      total_rewards += reward\n",
        "      \n",
        "      if done:\n",
        "        print(\"Score\", total_rewards)\n",
        "        total_test_rewards.append(total_rewards)\n",
        "        break\n",
        "        \n",
        "      next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "      state = next_state\n",
        "      \n",
        "  env.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
            "******************************************************\n",
            "EPISODE  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dJsf1ZxlkORz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}